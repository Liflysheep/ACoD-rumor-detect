{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dff4613b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本: '\n",
      "      1 SC_val推理：消息缺乏明确权威来源（如巴黎官方或埃菲尔铁塔管理方声明），使用话题标签表明源自社交媒体而非权威新闻。得分：0.15\n",
      "      2 MC_val推理：无独立消息源佐证（如法国媒体、国际新闻机构），可靠渠道均未支持铁塔亮灯说法。得分：0.1\n",
      "      3 ES_val推理：视觉证据存在篡改或误用嫌疑，无可靠记录证明该灯光事件，与拉合尔袭击的关联具推测性。得分：0.05\n",
      "      计算：(0.15+0.1+0.05)/3=0.1\n",
      "      #### 0.1'\n",
      "Token数量: 147\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from typing import List\n",
    "\n",
    "# 全局变量，用于存储分词器，避免重复加载\n",
    "_tokenizer = None\n",
    "\n",
    "def get_tokenizer():\n",
    "    \"\"\"\n",
    "    获取或初始化分词器。\n",
    "    分词器只加载一次，以提高效率。\n",
    "    \"\"\"\n",
    "    global _tokenizer\n",
    "    if _tokenizer is None:\n",
    "        # 请确保 './utils/' 路径下有您预训练模型的分词器文件\n",
    "        # 例如：tokenizer.json, vocab.txt 等\n",
    "        _tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            \"./utils/\",\n",
    "            trust_remote_code=True,\n",
    "            use_fast=True  # 强制使用快速分词器\n",
    "        )\n",
    "    return _tokenizer\n",
    "\n",
    "def calculate_token_count(text: str) -> int:\n",
    "    \"\"\"\n",
    "    计算给定字符串的token数量\n",
    "    \n",
    "    Args:\n",
    "        text (str): 要计算token数量的文本\n",
    "        \n",
    "    Returns:\n",
    "        int: 文本的token数量\n",
    "    \"\"\"\n",
    "    tokenizer = get_tokenizer()\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    return len(tokens)\n",
    "\n",
    "# 示例使用\n",
    "if __name__ == \"__main__\":\n",
    "    # 自定义字符串\n",
    "    my_text = \"\"\"\n",
    "      1 SC_val推理：消息缺乏明确权威来源（如巴黎官方或埃菲尔铁塔管理方声明），使用话题标签表明源自社交媒体而非权威新闻。得分：0.15\n",
    "      2 MC_val推理：无独立消息源佐证（如法国媒体、国际新闻机构），可靠渠道均未支持铁塔亮灯说法。得分：0.1\n",
    "      3 ES_val推理：视觉证据存在篡改或误用嫌疑，无可靠记录证明该灯光事件，与拉合尔袭击的关联具推测性。得分：0.05\n",
    "      计算：(0.15+0.1+0.05)/3=0.1\n",
    "      #### 0.1\"\"\"\n",
    "    \n",
    "    # 计算token数量\n",
    "    token_count = calculate_token_count(my_text)\n",
    "    \n",
    "    # 输出结果\n",
    "    print(f\"文本: '{my_text}'\")\n",
    "    print(f\"Token数量: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e60efa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功将token数量保存到 './data/twitter_token.txt'。共处理了 14355 行文本。\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from typing import List\n",
    "\n",
    "# 全局变量，用于存储分词器，避免重复加载\n",
    "_tokenizer = None\n",
    "\n",
    "def get_tokenizer():\n",
    "    \"\"\"\n",
    "    获取或初始化分词器。\n",
    "    分词器只加载一次，以提高效率。\n",
    "    \"\"\"\n",
    "    global _tokenizer\n",
    "    if _tokenizer is None:\n",
    "        # 请确保 './utils/' 路径下有您预训练模型的分词器文件\n",
    "        # 例如：tokenizer.json, vocab.txt 等\n",
    "        _tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            \"./utils/\",\n",
    "            trust_remote_code=True,\n",
    "            use_fast=True  # 强制使用快速分词器\n",
    "        )\n",
    "    return _tokenizer\n",
    "\n",
    "def calculate_and_save_token_counts(\n",
    "    input_filepath: str,\n",
    "    output_filepath: str\n",
    "):\n",
    "    \"\"\"\n",
    "    读取指定文件中的每行文本，计算每行文本的token数量，\n",
    "    并将结果保存到另一个文件中。\n",
    "\n",
    "    Args:\n",
    "        input_filepath (str): 输入文件的路径，每行包含一个文本。\n",
    "        output_filepath (str): 输出文件的路径，每行保存一个token数量。\n",
    "    \"\"\"\n",
    "    tokenizer = get_tokenizer()\n",
    "    token_counts = []\n",
    "\n",
    "    # 读取输入文件\n",
    "    try:\n",
    "        with open(input_filepath, 'r', encoding='utf-8') as infile:\n",
    "            for line in infile:\n",
    "                # 移除行末的换行符和多余的空白\n",
    "                text = line.strip()\n",
    "                if text: # 确保非空行才进行计算\n",
    "                    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "                    token_counts.append(len(tokens))\n",
    "                else:\n",
    "                    token_counts.append(0) # 空行或只有空白的行计为0 token\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 输入文件 '{input_filepath}' 未找到。请检查文件路径。\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"读取输入文件时发生错误: {e}\")\n",
    "        return\n",
    "\n",
    "    # 将token数量保存到输出文件\n",
    "    try:\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
    "            for count in token_counts:\n",
    "                outfile.write(str(count) + '\\n')\n",
    "        print(f\"成功将token数量保存到 '{output_filepath}'。共处理了 {len(token_counts)} 行文本。\")\n",
    "    except Exception as e:\n",
    "        print(f\"写入输出文件时发生错误: {e}\")\n",
    "\n",
    "# 示例使用\n",
    "if __name__ == \"__main__\":\n",
    "    # 调用函数处理文件\n",
    "    calculate_and_save_token_counts(\n",
    "        input_filepath=\"./data/twitter_lines.txt\",\n",
    "        output_filepath=\"./data/twitter_token.txt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db1f9cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chan spots a Boston Marathon bombings suspect</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So THIS is the face of the BostonMarathon bomb...</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dazonic U said it Appears that a Reddit user f...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Os caras do chan identificaram o terrorista da...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT YourAnonNews New FBI BostonMarathon Suspect...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14350</th>\n",
       "      <td>rapo Wow DylanByers RT DailyIntel The cover of...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14351</th>\n",
       "      <td>newyork hurricane sandy</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14352</th>\n",
       "      <td>Tomb of the Unknown Soldier during Hurricane S...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14353</th>\n",
       "      <td>NY The City and the Storm The Look of PostSand...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14354</th>\n",
       "      <td>Is that the Eiffel Tower lit up in Pakistan co...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14355 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label token\n",
       "0          chan spots a Boston Marathon bombings suspect      0     8\n",
       "1      So THIS is the face of the BostonMarathon bomb...      1    21\n",
       "2      dazonic U said it Appears that a Reddit user f...      1    22\n",
       "3      Os caras do chan identificaram o terrorista da...      0    19\n",
       "4      RT YourAnonNews New FBI BostonMarathon Suspect...      1    13\n",
       "...                                                  ...    ...   ...\n",
       "14350  rapo Wow DylanByers RT DailyIntel The cover of...      1    19\n",
       "14351                            newyork hurricane sandy      0     5\n",
       "14352  Tomb of the Unknown Soldier during Hurricane S...      0    13\n",
       "14353  NY The City and the Storm The Look of PostSand...      1    14\n",
       "14354  Is that the Eiffel Tower lit up in Pakistan co...      0    23\n",
       "\n",
       "[14355 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data():\n",
    "    data_rows = []  # 用于存储每一行数据的字典\n",
    "\n",
    "    try:\n",
    "        with (\n",
    "            open(\"./data/twitter_lines.txt\", \"r\", encoding=\"utf-8\") as text_file,\n",
    "            open(\"./data/twitter_label 0 for fake, 1 for true.txt\", \"r\", encoding=\"utf-8\") as label_file, # 修正：添加了逗号\n",
    "            open(\"./data/twitter_token.txt\", \"r\", encoding=\"utf-8\") as token_file\n",
    "        ):\n",
    "            for text_line, label_line, token_line in zip(text_file, label_file, token_file): # 修正：解包所有三个文件\n",
    "                text = text_line.strip()\n",
    "                label = int(label_line.strip())\n",
    "                token = token_line.strip() # 读取 token 数据\n",
    "\n",
    "                # 将每行数据存储为字典\n",
    "                row = {\n",
    "                    'text': text,\n",
    "                    'label': label,\n",
    "                    'token': token\n",
    "                }\n",
    "                data_rows.append(row)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        pass\n",
    "    # 从字典列表创建DataFrame\n",
    "    df = pd.DataFrame(data_rows)\n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea51fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label  token_count\n",
      "121                               PrayForAmerica Sandy      1            4\n",
      "136                                 hurricane sandy NY      0            4\n",
      "180                                     HurricaneSandy      0            4\n",
      "104                             This Morning NYC Sandy      1            4\n",
      "111                                      Sandy at work      0            4\n",
      "..                                                 ...    ...          ...\n",
      "112  RT BREAKlNG WANTED  yrold Dzhokhar TsarnaevSus...      1           31\n",
      "137  Hoy os estn colando multitud de fotomontajes d...      0           31\n",
      "195  aparmisen No s si esta foto del temporal en NY...      0           33\n",
      "166  RT odtumuhalefet Gericilikten katillerden kork...      0           45\n",
      "191  Fuji created huge lenticular clouds and they w...      1           54\n",
      "\n",
      "[200 rows x 3 columns]\n",
      "\n",
      "Label分布:\n",
      "label\n",
      "0    0.54\n",
      "1    0.46\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Token数量统计:\n",
      "count    200.000000\n",
      "mean      16.110000\n",
      "std        7.519349\n",
      "min        4.000000\n",
      "25%       11.000000\n",
      "50%       16.000000\n",
      "75%       21.000000\n",
      "max       54.000000\n",
      "Name: token_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    data_rows = []  # 用于存储每一行数据的字典\n",
    "    try:\n",
    "        with (\n",
    "            open(\"./data/twitter_lines.txt\", \"r\", encoding=\"utf-8\") as text_file,\n",
    "            open(\"./data/twitter_label 0 for fake, 1 for true.txt\", \"r\", encoding=\"utf-8\") as label_file,\n",
    "            open(\"./data/twitter_token.txt\", \"r\", encoding=\"utf-8\") as token_file\n",
    "        ):\n",
    "            for text_line, label_line, token_line in zip(text_file, label_file, token_file):\n",
    "                text = text_line.strip()\n",
    "                label = int(label_line.strip())\n",
    "                token_count = int(token_line.strip())  # 直接读取token数量\n",
    "                \n",
    "                row = {\n",
    "                    'text': text,\n",
    "                    'label': label,\n",
    "                    'token_count': token_count  # 直接存储token数量\n",
    "                }\n",
    "                data_rows.append(row)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"文件未找到: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame(data_rows)\n",
    "    return df\n",
    "\n",
    "def sample_by_token_count(df, n=200):\n",
    "    # 只保留 token_count <= 100 的行（效果等同于删除 >100 的行）\n",
    "    df1 = df[df['token_count'] <= 100]\n",
    "    df2 = df1[df1['token_count'] > 3]\n",
    "    # 1. 计算label比例\n",
    "    label_counts = df2['label'].value_counts(normalize=True)\n",
    "    \n",
    "    # 2. 按label分组\n",
    "    df_0 = df2[df2['label'] == 0].copy()\n",
    "    df_1 = df2[df2['label'] == 1].copy()\n",
    "    \n",
    "    # 3. 计算每组应该抽取的样本数（保持原始比例）\n",
    "    n_0 = int(n * label_counts[0])\n",
    "    n_1 = n - n_0  # 确保总数是200\n",
    "    \n",
    "    # 4. 对每组按token_count排序并均匀抽样\n",
    "    def stratified_sample(group, size):\n",
    "        if len(group) == 0:\n",
    "            return group\n",
    "        # 先按token_count排序\n",
    "        group = group.sort_values('token_count')\n",
    "        # 计算间隔以确保均匀分布\n",
    "        step = max(1, len(group) // size)\n",
    "        indices = np.linspace(0, len(group)-1, size, dtype=int)\n",
    "        return group.iloc[indices]\n",
    "    \n",
    "    sample_0 = stratified_sample(df_0, n_0)\n",
    "    sample_1 = stratified_sample(df_1, n_1)\n",
    "    \n",
    "    # 5. 合并样本\n",
    "    result = pd.concat([sample_0, sample_1]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 加载数据\n",
    "df = load_data()\n",
    "\n",
    "# 抽样200行\n",
    "sampled_df = sample_by_token_count(df, 200)\n",
    "\n",
    "# 按token_count升序排序\n",
    "sampled_df.sort_values(\"token_count\", ascending=True, inplace=True)\n",
    "\n",
    "# 显示结果\n",
    "print(sampled_df)\n",
    "print(\"\\nLabel分布:\")\n",
    "print(sampled_df['label'].value_counts(normalize=True))\n",
    "print(\"\\nToken数量统计:\")\n",
    "print(sampled_df['token_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c12a30f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成: ./data/tweets/test_nonrumor.txt → ./data/tweets/token_count_test_nonrumor.txt (996条)\n",
      "处理完成: ./data/tweets/test_rumor.txt → ./data/tweets/token_count_test_rumor.txt (1000条)\n",
      "处理完成: ./data/tweets/train_nonrumor.txt → ./data/tweets/token_count_train_nonrumor.txt (3783条)\n",
      "处理完成: ./data/tweets/train_rumor.txt → ./data/tweets/token_count_train_rumor.txt (3748条)\n",
      "\n",
      "处理完成！共处理 4 个文件\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "# 全局分词器\n",
    "_tokenizer = None\n",
    "\n",
    "def get_tokenizer():\n",
    "    global _tokenizer\n",
    "    if _tokenizer is None:\n",
    "        _tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            \"./utils/\",\n",
    "            trust_remote_code=True,\n",
    "            use_fast=True\n",
    "        )\n",
    "    return _tokenizer\n",
    "\n",
    "def clean_weibo_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    微博文本专用清洗：\n",
    "    1. 保留中文、英文、数字\n",
    "    2. 移除URL、@提及、话题标签\n",
    "    3. 移除所有标点符号（包括中文标点）\n",
    "    4. 合并连续空白\n",
    "    \"\"\"\n",
    "    # 移除URL和@提及\n",
    "    text = re.sub(r'http\\S+|@\\S+|#\\S+', '', text)\n",
    "    \n",
    "    # 只保留中文、英文、数字和空格\n",
    "    # 中文Unicode范围：\\u4e00-\\u9fa5\n",
    "    # 英文数字：a-zA-Z0-9\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # 合并连续空白并去除首尾空白\n",
    "    text = ' '.join(text.split()).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_weibo_file(input_filepath: str, output_filepath: str):\n",
    "    tokenizer = get_tokenizer()\n",
    "    token_counts = []\n",
    "\n",
    "    try:\n",
    "        with open(input_filepath, 'r', encoding='utf-8') as infile:\n",
    "            lines = infile.readlines()\n",
    "            \n",
    "            for i in range(2, len(lines), 3):  # 每3行取第3行\n",
    "                text = lines[i].strip()\n",
    "                text = clean_weibo_text(text)\n",
    "                \n",
    "                if text:\n",
    "                    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "                    token_counts.append(len(tokens))\n",
    "                else:\n",
    "                    token_counts.append(0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"处理文件 {input_filepath} 时出错: {e}\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
    "            outfile.write('\\n'.join(map(str, token_counts)))\n",
    "        print(f\"处理完成: {input_filepath} → {output_filepath} ({len(token_counts)}条)\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"写入 {output_filepath} 时出错: {e}\")\n",
    "        return False\n",
    "\n",
    "def process_weibo_dataset(input_dir: str, output_dir: str):\n",
    "    \"\"\"处理微博数据集\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    processed = 0\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.txt') and not filename.startswith('token_count_'):\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            output_path = os.path.join(output_dir, f\"token_count_{filename}\")\n",
    "            \n",
    "            if process_weibo_file(input_path, output_path):\n",
    "                processed += 1\n",
    "\n",
    "    print(f\"\\n处理完成！共处理 {processed} 个文件\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 示例使用\n",
    "    process_weibo_dataset(\n",
    "        input_dir=\"./data/tweets/\",\n",
    "        output_dir=\"./data/tweets/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72928dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抽样结果（前5行）：\n",
      "     label  token_count                                               text\n",
      "0        0           31  请国家还原真相nbspnbsp因为相关部门的过失害死了这么多人nbspnbspnbsp现在连...\n",
      "102      1           31  Stark家族恭喜权力的游戏在第67届艾美奖中获得24项提名与12座奖杯私生子囧斯诺则表示我...\n",
      "103      1           32  今天上午山东平邑一石膏矿垮塌引发震动目前有多人被埋伤亡不明据 正式测定振动相当于4级地震据新...\n",
      "1        0           32  转发一位叫徐敬的女孩21岁请速回雅安水城县人民医院妈妈伤的很严重想见她最后一面爸爸号码151...\n",
      "2        0           33  男主已被乱棍打死好么 来自网易新闻广西南宁一奔驰车闯拆迁现场撞人 致3人受伤更多精彩尽在 O...\n",
      "\n",
      "Label分布:\n",
      "label\n",
      "0    0.51\n",
      "1    0.49\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Token数量统计:\n",
      "count    200.000000\n",
      "mean      69.610000\n",
      "std       16.295571\n",
      "min       31.000000\n",
      "25%       61.000000\n",
      "50%       72.000000\n",
      "75%       80.000000\n",
      "max      125.000000\n",
      "Name: token_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "def clean_weibo_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    微博文本专用清洗：\n",
    "    1. 保留中文、英文、数字\n",
    "    2. 移除URL、@提及、话题标签\n",
    "    3. 移除所有标点符号（包括中文标点）\n",
    "    4. 合并连续空白\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 移除URL和@提及\n",
    "    text = re.sub(r'http\\S+|@\\S+|#\\S+', '', text)\n",
    "    \n",
    "    # 只保留中文、英文、数字和空格\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # 合并连续空白并去除首尾空白\n",
    "    text = ' '.join(text.split()).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def load_tweets_data():\n",
    "    data_rows = []  # 用于存储所有数据的列表\n",
    "    \n",
    "    # 定义要处理的文件列表及其对应标签\n",
    "    tweet_files = [\n",
    "        ('test_nonrumor.txt', 1),\n",
    "        ('test_rumor.txt', 0),\n",
    "        ('train_nonrumor.txt', 1),\n",
    "        ('train_rumor.txt', 0)\n",
    "    ]\n",
    "    \n",
    "    # 处理每个文件\n",
    "    for filename, label in tweet_files:\n",
    "        text_filepath = os.path.join('./data/tweets/', filename)\n",
    "        token_filepath = os.path.join('./data/tweets/', f'token_count_{filename}')\n",
    "        \n",
    "        try:\n",
    "            # 读取文本文件（每3行取第3行）并进行清洗\n",
    "            with open(text_filepath, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                texts = [clean_weibo_text(lines[i].strip()) for i in range(2, len(lines), 3)]\n",
    "            \n",
    "            # 读取对应的token计数文件\n",
    "            with open(token_filepath, 'r', encoding='utf-8') as f:\n",
    "                tokens = [int(line.strip()) for line in f.readlines()]\n",
    "            \n",
    "            # 确保文本和token数量匹配\n",
    "            min_len = min(len(texts), len(tokens))\n",
    "            for i in range(min_len):\n",
    "                data_rows.append({\n",
    "                    'label': label,\n",
    "                    'token_count': tokens[i],\n",
    "                    'text': texts[i],          # 存储清洗后的文本\n",
    "                    'original_text': lines[2+i*3].strip() if i*3+2 < len(lines) else \"\"  # 可选：存储原始文本\n",
    "                })\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"文件未找到: {text_filepath} 或 {token_filepath}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"处理文件 {text_filepath} 时出错: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(data_rows)\n",
    "    return df\n",
    "\n",
    "def sample_by_token_count(df, n=200):\n",
    "    # 只保留 token_count > 30 的行\n",
    "    df_filtered = df[df['token_count'] > 30].copy()\n",
    "    \n",
    "    # 1. 计算label比例\n",
    "    label_counts = df_filtered['label'].value_counts(normalize=True)\n",
    "    \n",
    "    # 2. 按label分组\n",
    "    df_0 = df_filtered[df_filtered['label'] == 0].copy()\n",
    "    df_1 = df_filtered[df_filtered['label'] == 1].copy()\n",
    "    \n",
    "    # 3. 计算每组应该抽取的样本数（保持原始比例）\n",
    "    n_0 = int(n * label_counts[0])\n",
    "    n_1 = n - n_0  # 确保总数是n\n",
    "    \n",
    "    # 4. 对每组按token_count排序并均匀抽样\n",
    "    def stratified_sample(group, size):\n",
    "        if len(group) == 0:\n",
    "            return group\n",
    "        group = group.sort_values('token_count')\n",
    "        indices = np.linspace(0, len(group)-1, size, dtype=int)\n",
    "        return group.iloc[indices]\n",
    "    \n",
    "    sample_0 = stratified_sample(df_0, n_0)\n",
    "    sample_1 = stratified_sample(df_1, n_1)\n",
    "    \n",
    "    # 5. 合并样本\n",
    "    result = pd.concat([sample_0, sample_1]).reset_index(drop=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载并清洗数据\n",
    "    df = load_tweets_data()\n",
    "    \n",
    "    # 抽样200行\n",
    "    sampled_df = sample_by_token_count(df, 200)\n",
    "    \n",
    "    # 按token_count升序排序\n",
    "    sampled_df.sort_values(\"token_count\", ascending=True, inplace=True)\n",
    "    \n",
    "    # 显示结果\n",
    "    print(\"抽样结果（前5行）：\")\n",
    "    print(sampled_df[['label', 'token_count', 'text']].head())\n",
    "    \n",
    "    print(\"\\nLabel分布:\")\n",
    "    print(sampled_df['label'].value_counts(normalize=True))\n",
    "    \n",
    "    print(\"\\nToken数量统计:\")\n",
    "    print(sampled_df['token_count'].describe())\n",
    "    \n",
    "    # 可选：保存清洗后的数据\n",
    "    sampled_df.to_csv('./data/cleaned_sampled_data.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ceecfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在分析数据集: twitter\n",
      "完成twitter分析，耗时0.36秒\n",
      "\n",
      "正在分析数据集: weibo\n",
      "完成weibo分析，耗时0.31秒\n",
      "\n",
      "分析完成! 结果已保存至: ./analysis/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import chi2_contingency\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# 配置参数\n",
    "PROMPTS = [\"cod\", \"cot\",\"baseline\", \"basebaseline\"]  # 使用的提示策略列表\n",
    "DATASETS = [\"twitter\",\"weibo\"]                         # 要分析的数据集列表\n",
    "# MODEL = \"deepseek-v3\"                          # 使用的模型名称\n",
    "MODEL = \"qwen2.5-32b\"                          # 使用的模型名称\n",
    "SHOT = 2                                       # few-shot数量\n",
    "OUTPUT_DIR = \"./analysis/\"                     # 输出目录\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)         # 确保输出目录存在\n",
    "\n",
    "def calculate_phi(confusion_matrix):\n",
    "    \"\"\"计算两个二分类变量的Phi相关系数\n",
    "    参数:\n",
    "        confusion_matrix: 混淆矩阵(列联表)\n",
    "    返回:\n",
    "        phi系数(范围[-1,1])\n",
    "    \"\"\"\n",
    "    # 计算卡方值\n",
    "    chi2, _, _, _ = chi2_contingency(confusion_matrix, correction=False)\n",
    "    n = confusion_matrix.sum().sum()  # 总样本数\n",
    "    # 计算phi系数并保留方向性\n",
    "    return np.sqrt(chi2 / n) * np.sign(confusion_matrix.iloc[0,0] * confusion_matrix.iloc[1,1] - \n",
    "                                      confusion_matrix.iloc[0,1] * confusion_matrix.iloc[1,0])\n",
    "\n",
    "def load_predictions(dataset, prompt):\n",
    "    \"\"\"加载预测数据及其所有可用字段\n",
    "    参数:\n",
    "        dataset: 数据集名称\n",
    "        prompt: 提示策略名称\n",
    "    返回:\n",
    "        包含所有预测数据的DataFrame\n",
    "    \"\"\"\n",
    "    file_path = f\"./results/{dataset}-{MODEL}-{prompt}-{SHOT}.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        # 确保必须的列存在\n",
    "        if 'predict' not in df.columns or 'gt' not in df.columns:\n",
    "            raise ValueError(\"CSV文件缺少必要列(predict/gt)\")\n",
    "        \n",
    "        # 添加衍生字段\n",
    "        df['is_correct'] = df['predict'] == df['gt']  # 预测是否正确\n",
    "        \n",
    "        # 处理时间字段(如果存在)\n",
    "        if 'time' in df.columns:\n",
    "            df['time'] = pd.to_numeric(df['time'], errors='coerce')  # 转换为数值\n",
    "            \n",
    "        # 处理输入token数(如果存在)\n",
    "        if 'qs_token' in df.columns:\n",
    "            df['qs_token'] = pd.to_numeric(df['qs_token'], errors='coerce')\n",
    "            \n",
    "        # 处理输出token数(如果存在)\n",
    "        if 'out_token' in df.columns:\n",
    "            df['out_token'] = pd.to_numeric(df['out_token'], errors='coerce')\n",
    "            \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"加载{file_path}出错: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def calculate_pairwise_metrics(predictions_dict):\n",
    "    \"\"\"计算所有提示策略两两之间的指标\n",
    "    参数:\n",
    "        predictions_dict: 包含各提示策略预测结果的字典\n",
    "    返回:\n",
    "        agreement_matrix: 一致性矩阵(百分比)\n",
    "        phi_matrix: phi系数矩阵(百分比)\n",
    "    \"\"\"\n",
    "    prompts = list(predictions_dict.keys())  # 获取所有提示策略名称\n",
    "    # 初始化结果矩阵\n",
    "    agreement_matrix = pd.DataFrame(index=prompts, columns=prompts, dtype=float)\n",
    "    phi_matrix = pd.DataFrame(index=prompts, columns=prompts, dtype=float)\n",
    "    \n",
    "    # 计算两两指标\n",
    "    for p1 in prompts:\n",
    "        for p2 in prompts:\n",
    "            df1 = predictions_dict[p1]\n",
    "            df2 = predictions_dict[p2]\n",
    "            \n",
    "            # 计算一致性百分比(预测结果相同的比例)\n",
    "            agreement = (df1['predict'] == df2['predict']).mean() * 100\n",
    "            agreement_matrix.loc[p1, p2] = agreement\n",
    "            \n",
    "            # 计算phi系数\n",
    "            confusion = pd.crosstab(df1['predict'], df2['predict'])  # 构建列联表\n",
    "            if confusion.shape == (2, 2):  # 确保是2x2矩阵\n",
    "                phi = calculate_phi(confusion) * 100\n",
    "            else:  # 如果某一类全为0/1时的处理\n",
    "                phi = 100 if (df1['predict'] == df2['predict']).all() else 0\n",
    "            phi_matrix.loc[p1, p2] = phi\n",
    "    \n",
    "    return agreement_matrix, phi_matrix\n",
    "\n",
    "def calculate_detailed_stats(predictions_dict):\n",
    "    \"\"\"为每个提示策略计算详细的统计信息（优化版）\n",
    "    参数:\n",
    "        predictions_dict: 包含各提示策略预测结果的字典\n",
    "    返回:\n",
    "        包含详细统计信息的DataFrame\n",
    "    \n",
    "    修改说明:\n",
    "        1. 将混淆矩阵分量改为百分比形式\n",
    "        2. 时间统计只保留平均值和标准差\n",
    "        3. token统计只保留输出token的平均值和标准差\n",
    "    \"\"\"\n",
    "    stats = []  # 存储各提示策略的统计信息\n",
    "    \n",
    "    for prompt, df in predictions_dict.items():\n",
    "        if df is None:  # 跳过加载失败的情况\n",
    "            continue\n",
    "        \n",
    "        total_samples = len(df)\n",
    "        \n",
    "        # 基础统计信息\n",
    "        stat = {\n",
    "            'prompt': prompt,  # 提示策略名称\n",
    "            'accuracy': df['is_correct'].mean() * 100,  # 准确率(百分比)\n",
    "            'num_samples': total_samples,  # 样本数量\n",
    "            \n",
    "            # 混淆矩阵各分量(改为百分比形式)\n",
    "            'true_positives_ratio': ((df['predict'] == 1) & (df['gt'] == 1)).sum() / total_samples * 100,\n",
    "            'false_positives_ratio': ((df['predict'] == 1) & (df['gt'] == 0)).sum() / total_samples * 100,\n",
    "            'true_negatives_ratio': ((df['predict'] == 0) & (df['gt'] == 0)).sum() / total_samples * 100,\n",
    "            'false_negatives_ratio': ((df['predict'] == 0) & (df['gt'] == 1)).sum() / total_samples * 100,\n",
    "            \n",
    "            # 保留原始计数字段(如果需要)\n",
    "            'true_positives': ((df['predict'] == 1) & (df['gt'] == 1)).sum(),\n",
    "            'false_positives': ((df['predict'] == 1) & (df['gt'] == 0)).sum(),\n",
    "            'true_negatives': ((df['predict'] == 0) & (df['gt'] == 0)).sum(),\n",
    "            'false_negatives': ((df['predict'] == 0) & (df['gt'] == 1)).sum(),\n",
    "        }\n",
    "        \n",
    "        # 时间统计(只保留平均值和标准差)\n",
    "        if 'time' in df.columns:\n",
    "            stat.update({\n",
    "                'avg_time': df['time'].mean(),  # 平均时间\n",
    "                'time_std': df['time'].std()    # 时间标准差\n",
    "            })\n",
    "        \n",
    "        # Token统计(只保留输出token的平均值和标准差)\n",
    "        if 'out_token' in df.columns:\n",
    "            stat.update({\n",
    "                'avg_output_tokens': df['out_token'].mean(),  # 平均输出token数\n",
    "                'std_output_tokens': df['out_token'].std()    # 输出token标准差\n",
    "            })\n",
    "        \n",
    "        stats.append(stat)\n",
    "    \n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "def analyze_dataset(dataset):\n",
    "    \"\"\"对单个数据集进行综合分析\n",
    "    参数:\n",
    "        dataset: 数据集名称\n",
    "    返回:\n",
    "        该数据集的详细统计信息\n",
    "    \"\"\"\n",
    "    print(f\"\\n正在分析数据集: {dataset}\")\n",
    "    start_time = time.time()  # 记录开始时间\n",
    "    \n",
    "    # 加载所有提示策略的预测结果\n",
    "    predictions = {prompt: load_predictions(dataset, prompt) for prompt in PROMPTS}\n",
    "    \n",
    "    # 计算各项指标\n",
    "    agreement_matrix, phi_matrix = calculate_pairwise_metrics(predictions)  # 一致性矩阵和phi矩阵\n",
    "    detailed_stats = calculate_detailed_stats(predictions)  # 详细统计信息\n",
    "    \n",
    "    # 保存结果\n",
    "    save_results(dataset, {\n",
    "        'agreement_matrix': agreement_matrix,\n",
    "        'phi_matrix': phi_matrix,\n",
    "        'detailed_stats': detailed_stats\n",
    "    })\n",
    "    \n",
    "    # 打印耗时信息\n",
    "    print(f\"完成{dataset}分析，耗时{time.time()-start_time:.2f}秒\")\n",
    "    return detailed_stats\n",
    "\n",
    "def save_results(dataset, results):\n",
    "    \"\"\"保存数据集的分析结果到文件\n",
    "    参数:\n",
    "        dataset: 数据集名称\n",
    "        results: 包含各种分析结果的字典\n",
    "    \"\"\"\n",
    "    prefix = f\"{dataset}_{MODEL}\"  # 文件名前缀\n",
    "    \n",
    "    # 保存矩阵数据(一致性和phi矩阵)\n",
    "    for name, matrix in [('agreement', results['agreement_matrix']), \n",
    "                        ('phi', results['phi_matrix'])]:\n",
    "        if matrix is not None:\n",
    "            df = matrix.reset_index().rename(columns={'index': 'prompt'})\n",
    "            df['dataset'] = dataset  # 添加数据集列\n",
    "            df.to_csv(os.path.join(OUTPUT_DIR, f\"{prefix}_{name}.csv\"), \n",
    "                     index=False, float_format=\"%.2f\")  # 保留两位小数\n",
    "    \n",
    "    # 保存详细统计数据\n",
    "    if results['detailed_stats'] is not None:\n",
    "        results['detailed_stats'].to_csv(\n",
    "            os.path.join(OUTPUT_DIR, f\"{prefix}_stats.csv\"),\n",
    "            index=False, float_format=\"%.2f\"  # 保留两位小数\n",
    "        )\n",
    "\n",
    "def analyze_all_datasets():\n",
    "    \"\"\"主分析函数，处理所有数据集\"\"\"\n",
    "    all_stats = []  # 存储所有数据集的统计信息\n",
    "    \n",
    "    for dataset in DATASETS:\n",
    "        stats = analyze_dataset(dataset)  # 分析当前数据集\n",
    "        if stats is not None:\n",
    "            stats['dataset'] = dataset  # 添加数据集列\n",
    "            all_stats.append(stats)  # 添加到总统计列表\n",
    "    \n",
    "    # 合并并保存所有数据集的统计信息\n",
    "    if all_stats:\n",
    "        combined_stats = pd.concat(all_stats)\n",
    "        combined_stats.to_csv(\n",
    "            os.path.join(OUTPUT_DIR, f\"{MODEL}_combined_stats.csv\"),\n",
    "            index=False, float_format=\"%.2f\"  # 保留两位小数\n",
    "        )\n",
    "    \n",
    "    print(\"\\n分析完成! 结果已保存至:\", OUTPUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_all_datasets()  # 执行分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c7f51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在处理数据集: twitter\n",
      "数据集 twitter 的合并正确率: 63.52%\n",
      "\n",
      "分析完成！合并正确率结果已保存至: ./analysis/qwen2.5-32b_merged_accuracy.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 配置参数\n",
    "PROMPTS = [\"cod\", \"baseline\", \"basebaseline\"]\n",
    "DATASETS = [\"twitter\"]\n",
    "MODEL = \"qwen2.5-32b\"\n",
    "SHOT = 2\n",
    "OUTPUT_DIR = \"./analysis/\"\n",
    "\n",
    "def analyze_datasets():\n",
    "    # 初始化一个DataFrame来存储每个数据集的合并正确率\n",
    "    merged_accuracy_results = pd.DataFrame(columns=['Dataset', 'Merged_Accuracy'])\n",
    "\n",
    "    for dataset in DATASETS:\n",
    "        print(f\"\\n正在处理数据集: {dataset}\")\n",
    "\n",
    "        all_predictions = []\n",
    "        gt_labels = None\n",
    "\n",
    "        # 读取所有提示策略的预测结果\n",
    "        for prompt in PROMPTS:\n",
    "            file_path = f\"./results/{dataset}-{MODEL}-{prompt}-{SHOT}.csv\"\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                all_predictions.append(df['predict'].astype(int))\n",
    "                if gt_labels is None:\n",
    "                    gt_labels = df['gt'].astype(int)\n",
    "            except Exception as e:\n",
    "                print(f\"读取{dataset}-{prompt}失败: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if not all_predictions:\n",
    "            print(f\"没有为数据集 {dataset} 找到有效的预测结果。\")\n",
    "            continue\n",
    "\n",
    "        # 将所有预测结果合并成一个DataFrame\n",
    "        predictions_df = pd.DataFrame(all_predictions).T\n",
    "        predictions_df.columns = PROMPTS\n",
    "\n",
    "        # 使用投票法进行合并：选择出现次数最多的预测\n",
    "        # 如果有平局，这里默认选择第一个遇到的类别（例如0或1）\n",
    "        merged_predictions = predictions_df.mode(axis=1)[0]\n",
    "\n",
    "        # 计算合并后的正确率\n",
    "        if gt_labels is not None:\n",
    "            correct_predictions = (merged_predictions == gt_labels).sum()\n",
    "            total_predictions = len(gt_labels)\n",
    "            merged_accuracy = (correct_predictions / total_predictions) * 100\n",
    "            print(f\"数据集 {dataset} 的合并正确率: {merged_accuracy:.2f}%\")\n",
    "\n",
    "            # 将结果添加到汇总DataFrame\n",
    "            merged_accuracy_results.loc[len(merged_accuracy_results)] = [dataset, merged_accuracy]\n",
    "        else:\n",
    "            print(f\"数据集 {dataset} 缺少真实标签 (gt)。\")\n",
    "\n",
    "    # 保存最终合并正确率的表格\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    output_filename = os.path.join(OUTPUT_DIR, f\"{MODEL}_merged_accuracy.csv\")\n",
    "    merged_accuracy_results.to_csv(output_filename, index=False, float_format=\"%.2f\")\n",
    "    print(\"\\n分析完成！合并正确率结果已保存至:\", output_filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_datasets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
